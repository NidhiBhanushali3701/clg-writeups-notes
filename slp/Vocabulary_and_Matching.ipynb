{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "05_Vocabulary_and_Matching.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCGhZglXyOc4"
      },
      "source": [
        "# Vocabulary and Matching\n",
        "In this section we will identify and label specific phrases that match patterns we can define ourselves. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCVPPzSyyOc5"
      },
      "source": [
        "## Rule-based Matching\n",
        "spaCy offers a rule-matching tool called `Matcher` that allows you to build a library of token patterns, then match those patterns against a Doc object to return a list of found matches. You can match on any part of the token including text and annotations, and you can add multiple patterns to the same matcher."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzI6msb8yOc6"
      },
      "source": [
        "# Perform standard imports\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fxfE-CqyOdB"
      },
      "source": [
        "# Import the Matcher library\n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp9es2ghyOdF"
      },
      "source": [
        "<font color=green>Here `matcher` is an object that pairs to the current `Vocab` object. We can add and remove specific named matchers to `matcher` as needed.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrrVuszqyOdG"
      },
      "source": [
        "### Creating patterns\n",
        "In literature, the phrase 'solar power' might appear as one word or two, with or without a hyphen. In this section we'll develop a matcher named 'SolarPower' that finds all three:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bA_8q8fyOdG"
      },
      "source": [
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]\n",
        "pattern3 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]\n",
        "\n",
        "matcher.add('SolarPower', None, pattern1, pattern2, pattern3)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHsne3ciyOdL"
      },
      "source": [
        "Let's break this down:\n",
        "* `pattern1` looks for a single token whose lowercase text reads 'solarpower'\n",
        "* `pattern2` looks for two adjacent tokens that read 'solar' and 'power' in that order\n",
        "* `pattern3` looks for three adjacent tokens, with a middle token that can be any punctuation.<font color=green>*</font>\n",
        "\n",
        "<font color=green>\\* Remember that single spaces are not tokenized, so they don't count as punctuation.</font>\n",
        "<br>Once we define our patterns, we pass them into `matcher` with the name 'SolarPower', and set *callbacks* to `None` (more on callbacks later)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWSbz90syOdL"
      },
      "source": [
        "### Applying the matcher to a Doc object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIlXm1UnyOdM"
      },
      "source": [
        "doc = nlp('The Solar Power industry continues to grow as demand \\\n",
        "for solarpower increases. Solar-power cars are gaining popularity.')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tXMv-wJyOdQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24feac8e-2cf8-476f-cca9-a29ea90b7463"
      },
      "source": [
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51phmPF6yOdW"
      },
      "source": [
        "`matcher` returns a list of tuples. Each tuple contains an ID for the match, with start & end tokens that map to the span `doc[start:end]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43cDh648yOdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "195a4cdf-1c90-4eae-de94-d89228fb15ab"
      },
      "source": [
        "for match_id, start, end in found_matches:\n",
        "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "    span = doc[start:end]                    # get the matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8656102463236116519 SolarPower 1 3 Solar Power\n",
            "8656102463236116519 SolarPower 10 11 solarpower\n",
            "8656102463236116519 SolarPower 13 16 Solar-power\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCvptGT4yOdc"
      },
      "source": [
        "The `match_id` is simply the hash value of the `string_ID` 'SolarPower'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g7AU9NUyOdd"
      },
      "source": [
        "### Setting pattern options and quantifiers\n",
        "You can make token rules optional by passing an `'OP':'*'` argument. This lets us streamline our patterns list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElOGlNLkyOdd"
      },
      "source": [
        "# Redefine the patterns:\n",
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
        "\n",
        "# Remove the old patterns to avoid duplication:\n",
        "matcher.remove('SolarPower')\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('SolarPower', None, pattern1, pattern2)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muQ_mrZxyOdh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15cd5cf3-9eea-4d35-c17d-1afac1428c34"
      },
      "source": [
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brzsEhMEyOdl"
      },
      "source": [
        "This found both two-word patterns, with and without the hyphen!\n",
        "\n",
        "The following quantifiers can be passed to the `'OP'` key:\n",
        "<table><tr><th>OP</th><th>Description</th></tr>\n",
        "\n",
        "<tr ><td><span >\\!</span></td><td>Negate the pattern, by requiring it to match exactly 0 times</td></tr>\n",
        "<tr ><td><span >?</span></td><td>Make the pattern optional, by allowing it to match 0 or 1 times</td></tr>\n",
        "<tr ><td><span >\\+</span></td><td>Require the pattern to match 1 or more times</td></tr>\n",
        "<tr ><td><span >\\*</span></td><td>Allow the pattern to match zero or more times</td></tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3M3255vyOdm"
      },
      "source": [
        "### Be careful with lemmas!\n",
        "If we wanted to match on both 'solar power' and 'solar powered', it might be tempting to look for the *lemma* of 'powered' and expect it to be 'power'. This is not always the case! The lemma of the *adjective* 'powered' is still 'powered':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCfu1GbbyOdn"
      },
      "source": [
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LEMMA': 'power'}] # CHANGE THIS PATTERN\n",
        "\n",
        "# Remove the old patterns to avoid duplication:\n",
        "matcher.remove('SolarPower')\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('SolarPower', None, pattern1, pattern2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDL1S6ESyOds"
      },
      "source": [
        "doc2 = nlp('Solar-powered energy runs solar-powered cars.')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJpcqm0JyOdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "540c5e6e-67b7-445a-847c-903dd6da2053"
      },
      "source": [
        "found_matches = matcher(doc2)\n",
        "print(found_matches)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(8656102463236116519, 0, 3), (8656102463236116519, 5, 8)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoGi3wwSyOd0"
      },
      "source": [
        "<font color=green>The matcher found the first occurrence because the lemmatizer treated 'Solar-powered' as a verb, but not the second as it considered it an adjective.<br>For this case it may be better to set explicit token patterns.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnF5e-HzyOd1"
      },
      "source": [
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
        "pattern3 = [{'LOWER': 'solarpowered'}]\n",
        "pattern4 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'powered'}]\n",
        "\n",
        "# Remove the old patterns to avoid duplication:\n",
        "matcher.remove('SolarPower')\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('SolarPower', None, pattern1, pattern2, pattern3, pattern4)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR1xhebdyOd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32397c31-5b54-46bd-c3ed-c228c7ce6c51"
      },
      "source": [
        "found_matches = matcher(doc2)\n",
        "print(found_matches)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(8656102463236116519, 0, 3), (8656102463236116519, 5, 8)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lsRa6MeyOd9"
      },
      "source": [
        "## Other token attributes\n",
        "Besides lemmas, there are a variety of token attributes we can use to determine matching rules:\n",
        "<table><tr><th>Attribute</th><th>Description</th></tr>\n",
        "\n",
        "<tr ><td><span >`ORTH`</span></td><td>The exact verbatim text of a token</td></tr>\n",
        "<tr ><td><span >`LOWER`</span></td><td>The lowercase form of the token text</td></tr>\n",
        "<tr ><td><span >`LENGTH`</span></td><td>The length of the token text</td></tr>\n",
        "<tr ><td><span >`IS_ALPHA`, `IS_ASCII`, `IS_DIGIT`</span></td><td>Token text consists of alphanumeric characters, ASCII characters, digits</td></tr>\n",
        "<tr ><td><span >`IS_LOWER`, `IS_UPPER`, `IS_TITLE`</span></td><td>Token text is in lowercase, uppercase, titlecase</td></tr>\n",
        "<tr ><td><span >`IS_PUNCT`, `IS_SPACE`, `IS_STOP`</span></td><td>Token is punctuation, whitespace, stop word</td></tr>\n",
        "<tr ><td><span >`LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL`</span></td><td>Token text resembles a number, URL, email</td></tr>\n",
        "<tr ><td><span >`POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE`</span></td><td>The token's simple and extended part-of-speech tag, dependency label, lemma, shape</td></tr>\n",
        "<tr ><td><span >`ENT_TYPE`</span></td><td>The token's entity label</td></tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVwLzCJ7yOd-"
      },
      "source": [
        "### Token wildcard\n",
        "You can pass an empty dictionary `{}` as a wildcard to represent **any token**. For example, you might want to retrieve hashtags without knowing what might follow the `#` character:\n",
        ">`[{'ORTH': '#'}, {}]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTYYkhIwyOd-"
      },
      "source": [
        "___\n",
        "## PhraseMatcher\n",
        "In the above section we used token patterns to perform rule-based matching. An alternative - and often more efficient - method is to match on terminology lists. In this case we use PhraseMatcher to create a Doc object from a list of phrases, and pass that into `matcher` instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HRg4IbkyOd_"
      },
      "source": [
        "# Perform standard imports, reset nlp\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suZls2LCyOeD"
      },
      "source": [
        "# Import the PhraseMatcher library\n",
        "from spacy.matcher import PhraseMatcher\n",
        "matcher = PhraseMatcher(nlp.vocab)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByDiXRj4yOeJ"
      },
      "source": [
        "For this exercise we're going to import a Wikipedia article on *Reaganomics*<br>\n",
        "Source: https://en.wikipedia.org/wiki/Reaganomics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHIzTu0MyOeJ"
      },
      "source": [
        "with open('reaganomics.txt', encoding='cp1252') as f:\n",
        "    doc3 = nlp(f.read())"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eASdI0ioyOeN"
      },
      "source": [
        "# First, create a list of match phrases:\n",
        "phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']\n",
        "\n",
        "# Next, convert each phrase to a Doc object:\n",
        "phrase_patterns = [nlp(text) for text in phrase_list]\n",
        "\n",
        "# Pass each Doc object into matcher (note the use of the asterisk!):\n",
        "matcher.add('VoodooEconomics', None, *phrase_patterns)\n",
        "\n",
        "# Build a list of matches:\n",
        "matches = matcher(doc3)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMlm48Q5yOeR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5f82f2b-d5fb-455e-c051-fcc3cfee224f"
      },
      "source": [
        "# (match_id, start, end)\n",
        "matches"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3473369816841043438, 41, 45),\n",
              " (3473369816841043438, 49, 53),\n",
              " (3473369816841043438, 54, 56),\n",
              " (3473369816841043438, 61, 65),\n",
              " (3473369816841043438, 673, 677),\n",
              " (3473369816841043438, 2987, 2991)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKGiv-PFyOeV"
      },
      "source": [
        "<font color=green>The first four matches are where these terms are used in the definition of Reaganomics:</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcQIkmDQyOeW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "908312d5-b859-462c-c2bb-9d4a0c7798d6"
      },
      "source": [
        "doc3[:70]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "REAGANOMICS\n",
              "https://en.wikipedia.org/wiki/Reaganomics\n",
              "\n",
              "Reaganomics (a portmanteau of [Ronald] Reagan and economics attributed to Paul Harvey)[1] refers to the economic policies promoted by U.S. President Ronald Reagan during the 1980s. These policies are commonly associated with supply-side economics, referred to as trickle-down economics or voodoo economics by political opponents, and free-market economics by political advocates.\n"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxvy_QivyOeZ"
      },
      "source": [
        "## Viewing Matches\n",
        "There are a few ways to fetch the text surrounding a match. The simplest is to grab a slice of tokens from the doc that is wider than the match:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4bpNC5OyOea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "452649b8-5fc0-4540-fd36-88a4d7715665"
      },
      "source": [
        "doc3[665:685]  # Note that the fifth match starts at doc3[673]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "same time he attracted a following from the supply-side economics movement, which formed in opposition to Keynesian"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt4tFOktyOef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86b97938-8910-45b5-a00f-e4e4348d6ab3"
      },
      "source": [
        "doc3[2975:2995]  # The sixth match starts at doc3[2985]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ") lawsuits against institutions.[66] His policies became widely known as \"trickle-down economics\", due to"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBQJTIFyyOek"
      },
      "source": [
        "Another way is to first apply the `sentencizer` to the Doc, then iterate through the sentences to the match point:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKv33h_XyOel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "851527fb-16bd-4de8-db4f-de28f74e31eb"
      },
      "source": [
        "# Build a list of sentences\n",
        "sents = [sent for sent in doc3.sents]\n",
        "\n",
        "# In the next section we'll see that sentences contain start and end token values:\n",
        "print(sents[0].start, sents[0].end)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hCHL0hnyOep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2f309a4-fdb6-4ca6-dd03-4f85116c9cdc"
      },
      "source": [
        "# Iterate over the sentence list until the sentence end value exceeds a match start value:\n",
        "for sent in sents:\n",
        "    if matches[4][1] < sent.end:  # this is the fifth match, that starts at doc3[673]\n",
        "        print(sent)\n",
        "        break"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At the same time he attracted a following from the supply-side economics movement, which formed in opposition to Keynesian demand-stimulus economics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7PKhn3lyOet"
      },
      "source": [
        "For additional information visit https://spacy.io/usage/linguistic-features#section-rule-based-matching\n"
      ]
    }
  ]
}