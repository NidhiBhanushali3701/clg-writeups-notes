{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "01_Tokenization.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU3gfugCiymY"
      },
      "source": [
        "# Tokenization\n",
        "The first step in creating a `Doc` object is to break down the incoming text into component pieces or \"tokens\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkekJPW2iymZ"
      },
      "source": [
        "# Import spaCy and load the language library\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQfoszbyiymf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "404bb948-a0dc-42e9-b257-ebccb3e02a59"
      },
      "source": [
        "# Create a string that includes opening and closing quotation marks\n",
        "mystring = '\"We\\'re moving to L.A.!\"'\n",
        "print(mystring)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"We're moving to L.A.!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCI9KU_Jiymn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7a088e7-4a16-4c71-ded4-9c3b4ccd5e4f"
      },
      "source": [
        "# Create a Doc object and explore tokens\n",
        "doc = nlp(mystring)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, end=' | ')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" | We | 're | moving | to | L.A. | ! | \" | "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQF5YvS9iymu"
      },
      "source": [
        "Notice that tokens are pieces of the original text. That is, we don't see any conversion to word stems or lemmas (base forms of words) and we haven't seen anything about organizations/places/money etc. Tokens are the basic building blocks of a Doc object - everything that helps us understand the meaning of the text is derived from tokens and their relationship to one another."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mosfCcf7iymv"
      },
      "source": [
        "## Prefixes, Suffixes and Infixes\n",
        "spaCy will isolate punctuation that does *not* form an integral part of a word. Quotation marks, commas, and punctuation at the end of a sentence will be assigned their own token. However, punctuation that exists as part of an email address, website or numerical value will be kept as part of the token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpzwfYS7iymw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1658cd3-a708-4bdd-eaa6-bfb8f495f23f"
      },
      "source": [
        "doc2 = nlp(\"We're here to help! Send snail-mail, email support@oursite.com or visit us at http://www.oursite.com!\")\n",
        "\n",
        "for t in doc2:\n",
        "    print(t)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We\n",
            "'re\n",
            "here\n",
            "to\n",
            "help\n",
            "!\n",
            "Send\n",
            "snail\n",
            "-\n",
            "mail\n",
            ",\n",
            "email\n",
            "support@oursite.com\n",
            "or\n",
            "visit\n",
            "us\n",
            "at\n",
            "http://www.oursite.com\n",
            "!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1A1fXPYiym1"
      },
      "source": [
        "<font color=green>Note that the exclamation points, comma, and the hyphen in 'snail-mail' are assigned their own tokens, yet both the email address and website are preserved.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSp_5pcBiym2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ad17a1-537e-47a4-dae4-fc7f79f36b3e"
      },
      "source": [
        "doc3 = nlp('A 5km NYC cab ride costs $10.30')\n",
        "\n",
        "for t in doc3:\n",
        "    print(t)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "5\n",
            "km\n",
            "NYC\n",
            "cab\n",
            "ride\n",
            "costs\n",
            "$\n",
            "10.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j42nZQeuiym7"
      },
      "source": [
        "<font color=green>Here the distance unit and dollar sign are assigned their own tokens, yet the dollar amount is preserved.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYIJj6omiym8"
      },
      "source": [
        "## Exceptions\n",
        "Punctuation that exists as part of a known abbreviation will be kept as part of the token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mmckQ0Hiym9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c0eab5-36ef-4869-94e6-bfa04bb39893"
      },
      "source": [
        "doc4 = nlp(\"Let's visit St. Louis in the U.S. next year.\")\n",
        "\n",
        "for t in doc4:\n",
        "    print(t)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let\n",
            "'s\n",
            "visit\n",
            "St.\n",
            "Louis\n",
            "in\n",
            "the\n",
            "U.S.\n",
            "next\n",
            "year\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu_uJdA3iynC"
      },
      "source": [
        "<font color=green>Here the abbreviations for \"Saint\" and \"United States\" are both preserved.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEtjPl2PiynD"
      },
      "source": [
        "## Counting Tokens\n",
        "`Doc` objects have a set number of tokens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWzVhuZQiynE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b77470c-0c18-492a-cd2d-8ddfe3e768fb"
      },
      "source": [
        "len(doc)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lI9TqsxEiynI"
      },
      "source": [
        "## Counting Vocab Entries\n",
        "`Vocab` objects contain a full library of items!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lru8Tm-jiynJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c50c95a2-2db6-4c2f-e836-2890ac0f72c9"
      },
      "source": [
        "len(doc.vocab)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFzSp5u6iynO"
      },
      "source": [
        "<font color=green>NOTE: This number changes based on the language library loaded at the start, and any new lexemes introduced to the `vocab` when the `Doc` was created.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNM_VnNFiynP"
      },
      "source": [
        "## Tokens can be retrieved by index position and slice\n",
        "`Doc` objects can be thought of as lists of `token` objects. As such, individual tokens can be retrieved by index position, and spans of tokens can be retrieved through slicing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3SSQ38biynQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "252b04a9-bb45-413b-ce49-395d15343c3e"
      },
      "source": [
        "doc5 = nlp('It is better to give than to receive.')\n",
        "\n",
        "# Retrieve the third token:\n",
        "doc5[2]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "better"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QBM5AT-iynV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d52de7-7a7f-48c3-c6d2-7c1d2024a540"
      },
      "source": [
        "# Retrieve three tokens from the middle:\n",
        "doc5[2:5]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "better to give"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBpyP2iqiyna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fafdc79-b4e4-4edc-b69a-cb4289a46e24"
      },
      "source": [
        "# Retrieve the last four tokens:\n",
        "doc5[-4:]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "than to receive."
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CAcs6Ceiyne"
      },
      "source": [
        "## Tokens cannot be reassigned\n",
        "Although `Doc` objects can be considered lists of tokens, they do *not* support item reassignment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ghLuZoiiynf"
      },
      "source": [
        "doc6 = nlp('My dinner was horrible.')\n",
        "doc7 = nlp('Your dinner was delicious.')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRLt8gEsiynk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "10a326a0-c04e-4449-d606-35b39afd5ef5"
      },
      "source": [
        "# Try to change \"My dinner was horrible\" to \"My dinner was delicious\"\n",
        "doc6[3] = doc7[3]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d4fb8c39c40b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Try to change \"My dinner was horrible\" to \"My dinner was delicious\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdoc6\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc7\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'spacy.tokens.doc.Doc' object does not support item assignment"
          ]
        }
      ]
    }
  ]
}